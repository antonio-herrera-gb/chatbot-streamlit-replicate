{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Taller: **Construye un asistente de IA con Streamlit y Replicate**\n",
        "\n",
        "*Rodrigo Oliver*  \n",
        "*Lead Instructor Bootcamp Data Science Online*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introducción\n",
        "\n",
        "En este taller aprenderemos a crear un chatbot utilizando:\n",
        "- GitHub como plataforma de desarrollo\n",
        "- Replicate como proveedor de modelos de lenguaje\n",
        "- Streamlit como soporte de nuestra interfaz\n",
        "\n",
        "### Disclaimer\n",
        "\n",
        "El objetivo del taller es construir una POC o Prueba de Concepto sencilla en la que crearemos una interfaz personalizada para un modelo open-source de IA ya entrenado y disponible en internet.  \n",
        "\n",
        "**Importante, esta implementación NO está lista para producción**, y su propósito es demostrar conceptos fundamentales y proporcionar una base sólida para futuras iteraciones más robustas.\n",
        "\n",
        "### Requisitos previos\n",
        "- Una cuenta en [GitHub](https://github.com/signup)\n",
        "\n",
        "### Instalación\n",
        "\n",
        "Utiliza este repositorio como **plantilla**, esto copiará los archivos en tu perfil de GitHub.  \n",
        "\n",
        "Una vez tengas el repositorio en tu perfil, lo abriremos mediante Codespaces de GitHub que servirá como nuestra plataforma de desarrollo.\n",
        "GitHub te permite utilizar Codespaces de manera gratuita durante:\n",
        "- 120 horas al mes en instancias de 2 núcleos\n",
        "- 60 horas al mes en instancias de 4 núcleos\n",
        "\n",
        "Si lo prefieres, y sabes cómo hacerlo, también puedes descargarlo en tu equipo, para lo que te recomendamos que crees un entorno virtual e instales las dependencias del proyecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1: Cómo usar Replicate  \n",
        "\n",
        "Adaptado y actualizado de *\"How to use Llama 2\"* por Chanin Nantasenamat [https://blog.streamlit.io/author/chanin/](https://blog.streamlit.io/author/chanin/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **¿Qué es Replicate?**\n",
        "\n",
        "Replicate es una plataforma que facilita el uso de modelos de inteligencia artificial a través de APIs sencillas. Permite ejecutar modelos de código abierto en la nube sin necesidad de configurar infraestructura compleja o gestionar recursos computacionales.  \n",
        "\n",
        "### **¿Por qué Replicate?**\n",
        "\n",
        "Replicate ofrece acceso a cientos de modelos de IA (incluyendo LLMs como Llama 3, Llama 4, Claude, así como modelos de visión, generación de imágenes y audio, etc.) ocupándose totalmente de la infraestructura necesaria para ello.\n",
        "\n",
        "Esto es **extremadamente práctico**, ya que elimina la necesidad de configurar GPUs y servidores muy potentes y caros, proporcionando a cambio un sistema de pago por uso adaptado a cada modelo.\n",
        "\n",
        "Para ello, Replicate nos proporciona una interfaz unificada y simple para interactuar con diversos modelos mediante APIs sencillas, disponibles en diferentes lenguajes.\n",
        "\n",
        "[**Si no sabes qué es una API, pulsa aquí**](https://aws.amazon.com/es/what-is/api/)\n",
        "\n",
        "### **¿Cómo funciona Replicate?**\n",
        "\n",
        "De forma muy resumida:\n",
        "\n",
        "1. Exploras el catálogo de modelos disponibles y eliges el que mejor se adapte a tu caso de uso.  \n",
        "2. Incorporas llamadas a la API de Replicate en tu aplicación con unas pocas líneas de código.  \n",
        "3. Replicate ejecuta el modelo en sus servidores, gestionando toda la infraestructura necesaria.  \n",
        "4. Obtienes las respuestas generadas en tiempo real, con opciones para streaming en muchos modelos.  \n",
        "\n",
        "\n",
        "Para comenzar con Replicate solo necesitas:\n",
        "- Crear una cuenta en [replicate.com](https://replicate.com)\n",
        "- Generar tu API token personal\n",
        "- Instalar la biblioteca de Python en tu entorno local o virtual (`pip install replicate`)\n",
        "- Empezar a hacer llamadas a los modelos desde tu código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **¿Cómo ejecutar Replicate con Python?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIf3Q7QaK4gn"
      },
      "source": [
        "#### **Paso 1: Instalar la librería de Replicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting replicate\n",
            "  Downloading replicate-1.0.7-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from replicate) (0.27.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from replicate) (24.1)\n",
            "Collecting pydantic>1.10.7 (from replicate)\n",
            "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from replicate) (4.12.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (4.6.2.post1)\n",
            "Requirement already satisfied: certifi in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (1.0.6)\n",
            "Requirement already satisfied: idna in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (3.10)\n",
            "Requirement already satisfied: sniffio in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>1.10.7->replicate)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic>1.10.7->replicate)\n",
            "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic>1.10.7->replicate)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading replicate-1.0.7-py3-none-any.whl (48 kB)\n",
            "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.0/2.0 MB 10.9 MB/s eta 0:00:00\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, replicate\n",
            "Successfully installed annotated-types-0.7.0 pydantic-2.11.9 pydantic-core-2.33.2 replicate-1.0.7 typing-inspection-0.4.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install replicate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqBzUTg9NMdh"
      },
      "source": [
        "#### **Paso 2: Importar Replicate y fijar el API token en nuestro entorno**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ga2m-1FNP7o"
      },
      "outputs": [],
      "source": [
        "import replicate\n",
        "import os\n",
        "\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"tu_api_token\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Paso 3: Documentarnos sobre nuestro modelo**\n",
        "\n",
        "Para este taller hemos elegido el modelo Llama 3-8b-instruct de Meta por su equilibrio entre coste y rendimiento, pese a no tratarse del modelo más potente ni más vanguardista.\n",
        "\n",
        "Llama 3-8b-instruct es lo que se conoce como un *Instruction-Tuned LLM*, es decir, se basa en un modelo fundacional al que se ha sometido a técnicas de ajuste para entender y seguir instrucciones específicas.\n",
        "\n",
        "Toda la información del modelo está disponible en su página oficial de Replicate: [replicate.com/meta/meta-llama-3-8b-instruct](https://replicate.com/meta/meta-llama-3-8b-instruct)\n",
        "\n",
        "##### ¿Qué información encontrarás?\n",
        "\n",
        "- Detalles técnicos de este modelo de 8 billones de parámetros optimizado para chat.\n",
        "- Documentación sobre su arquitectura, entrenamiento y capacidades.\n",
        "- Ejemplos de uso con código de muestra para integrar el modelo en tus aplicaciones.\n",
        "- **Playground interactivo** para probar el modelo directamente en el navegador.\n",
        "- Información sobre el sistema de facturación por tokens (entrada/salida).\n",
        "- Limitaciones y consideraciones éticas, guías para un uso responsable.\n",
        "\n",
        "##### Precios (importante)\n",
        "\n",
        "El modelo elegido tiene un sistema de precios predecible basado en:\n",
        "- Cantidad de tokens de entrada enviados\n",
        "- Cantidad de tokens de salida generados\n",
        "\n",
        "Esta estructura de precios hace que el coste sea más predecible que los modelos facturados por tiempo de computación.\n",
        "\n",
        "[**Si no sabes qué es un token, pulsa aquí**](https://learn.microsoft.com/es-es/dotnet/ai/conceptual/understanding-tokens)\n",
        "\n",
        "##### **Pero si el modelo es open-source, ¿por qué tengo que pagar por usarlo?**\n",
        "\n",
        "**Por el coste de la infraestructura necesaria para ejecutarlo.**    \n",
        "\n",
        "Es cierto que los modelos open-source están disponibles para su descarga en plataformas como [HuggingFace](https://huggingface.co/), PERO para ejecutar un modelo es necesario tener una tarjeta GPU (¡o varias!) con memoria suficiente para instanciar el modelo completo (y eso sin contar si queremos ajustarlo mediante fine-tuning). \n",
        "\n",
        "Además del coste del hardware, también está la complejidad de configurar el entorno, gestionar actualizaciones, escalado automático en caso de desplegar en cloud, etc. Replicate se encarga de todo esto por ti.\n",
        "\n",
        "Existen alternativas a Replicate, por ejemplo HuggingFace también ofrece sus propios proveedores de inferencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901Hxea9K7ME"
      },
      "source": [
        "#### **Paso 4: Ejecutar el modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Métodos de ejecución del modelo Llama 3-8b-instruct\n",
        "\n",
        "Como seguro ya sabéis, la forma en la que nos comunicamos con los modelos de lenguaje es mediante los famosos **prompts**, pero en esta ocasión lo haremos a través de la API usando los métodos de su librería de Python.\n",
        "\n",
        "Para el caso de Llama 3-8b-instruct, tenemos tres métodos disponibles para obtener las predicciones:\n",
        "\n",
        "##### 1. Método `run` (más sencillo)\n",
        "\n",
        "- **Funcionamiento**: Envía una solicitud y espera hasta recibir la respuesta completa.\n",
        "- **Ventajas**: Simple de implementar, ideal para scripts y procesamientos por lotes.\n",
        "- **Desventajas**: Bloqueante, tu aplicación debe esperar hasta que se complete toda la generación.\n",
        "- **Ideal para**: Tareas donde necesitas el resultado completo antes de continuar o aplicaciones con procesamiento asíncrono propio.  \n",
        "\n",
        "```python\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        ")\n",
        "print(\"\".join(output))\n",
        "```\n",
        "\n",
        "##### 2. Método `stream` (el que nos interesa)\n",
        "\n",
        "- **Funcionamiento**: Recibe las respuestas del modelo fragmento a fragmento en tiempo real.\n",
        "- **Ventajas**: Proporciona feedback inmediato al usuario, mejora la experiencia de interacción.\n",
        "- **Desventajas**: Requiere manejar la lógica de streaming en tu front-end.\n",
        "- **Ideal para**: Chatbots e interfaces conversacionales donde quieres mostrar las respuestas a medida que se generan.  \n",
        "\n",
        "```python\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        "):\n",
        "    print(event, end=\"\")\n",
        "```\n",
        "\n",
        "##### 3. Webhooks (más avanzado)\n",
        "\n",
        "- **Funcionamiento**: Envías una solicitud y proporcionas una URL de callback donde recibirás la respuesta cuando esté lista.\n",
        "- **Ventajas**: Totalmente asíncrono, no bloquea recursos mientras espera, ideal para arquitecturas serverless.\n",
        "- **Desventajas**: Requiere configurar un endpoint accesible públicamente para recibir las respuestas.\n",
        "- **Ideal para**: Aplicaciones de alto rendimiento, procesamientos largos, o arquitecturas orientadas a eventos.\n",
        "\n",
        "```python\n",
        "prediction = replicate.predictions.create(\n",
        "    version=\"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"},\n",
        "    webhook=\"https://tu-aplicacion.com/webhook\"\n",
        ")\n",
        "```\n",
        "\n",
        "La elección entre estos métodos dependerá de tus necesidades y la arquitectura de tu aplicación.  \n",
        "\n",
        "A continuación tienes un ejemplo de uso de los métodos `run` y `stream`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "When it comes to machine learning models, the number of parameters refers to the number of learnable weights and biases in the model. In general, a higher number of parameters can indicate a more complex model that is capable of learning more intricate patterns in the data.\n",
            "\n",
            "However, having a larger number of parameters does not directly impact the speed of the model. Instead, the speed of the model is often determined by factors such as:\n",
            "\n",
            "1. Computational resources: The amount of memory and processing power available to the model can impact its speed. More complex models may require more computational resources to process.\n",
            "2. Algorithmic efficiency: The choice of algorithm and optimization techniques used to train the model can affect its speed. For example, some algorithms may be more efficient than others for certain types of data or model architectures.\n",
            "3. Hardware: The type of hardware used to run the model can also impact its speed. For example, models running on graphics processing units (GPUs) or tensor processing units (TPUs) may be faster than those running on central processing units (CPUs).\n",
            "\n",
            "In the case of Johnny and Tommy, having 8 billion parameters compared to 70 billion parameters may not necessarily make Tommy's model slower. However, it's possible that Tommy's model may require more computational resources or processing power to train and run, which could impact its speed.\n",
            "\n",
            "To give you a better idea, here are some rough estimates of the computational resources required for training large-scale models:\n",
            "\n",
            "* 8 billion parameters: ~10-20 GB of memory, ~1-5 GPU days to train\n",
            "* 70 billion parameters: ~50-100 GB of memory, ~10-50 GPU days to train\n",
            "\n",
            "Keep in mind that these are rough estimates and can vary greatly depending on the specific model architecture, data, and hardware used.\n"
          ]
        }
      ],
      "source": [
        "# Método run\n",
        "prompt =  \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. \\\n",
        "    What does this mean when it comes to speed?\"\n",
        "\n",
        "input = {\"prompt\": prompt}\n",
        "\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        ")\n",
        "\n",
        "print(\"\".join(output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "When we talk about the number of parameters in a neural network, it's often referred to as the model's complexity or size. In this case, Johnny's model has 8 billion parameters, while Tommy's model has 70 billion parameters.\n",
            "\n",
            "In general, a larger model with more parameters can be more powerful and capable of learning more complex patterns in the data. However, it also means that the model is more computationally expensive to train and run.\n",
            "\n",
            "When it comes to speed, a larger model with more parameters typically requires more computational resources and may take longer to train and make predictions. This is because the model has to process and update more parameters, which can lead to increased memory usage, slower training times, and higher computational costs.\n",
            "\n",
            "Here are some possible implications of Johnny's and Tommy's model sizes on speed:\n",
            "\n",
            "* Johnny's 8 billion-parameter model might be able to train and make predictions relatively quickly, using moderately powerful hardware. However, it may not be able to handle very large or complex datasets.\n",
            "* Tommy's 70 billion-parameter model, on the other hand, might require more powerful hardware and may take significantly longer to train and make predictions. This could be due to the increased computational requirements, memory usage, and potential for overfitting (where the model becomes too specialized to the training data and doesn't generalize well to new data).\n",
            "\n",
            "To give you a better idea, here are some rough estimates of the computational resources required for each model:\n",
            "\n",
            "* Johnny's 8 billion-parameter model might require:\n",
            "\t+ A few high-end GPUs (e.g., NVIDIA V100 or A100) or a few powerful CPUs (e.g., Intel Xeon or AMD EPYC)\n",
            "\t+ Around 10-20 GB of memory (RAM) for training and 1-2 GB for inference\n",
            "\t+ Training times ranging from a few hours to a few days, depending on the dataset and optimization techniques used\n",
            "* Tommy's 70 billion-parameter model might require:\n",
            "\t+ Multiple high-end GPUs (e.g., NVIDIA A100 or Tesla V100) or a cluster of powerful CPUs (e.g., Intel Xeon or AMD EPYC)\n",
            "\t+ Around 50-100 GB of memory (RAM) for training and 5-10 GB for inference\n",
            "\t+ Training times ranging from several days to several weeks or even months, depending on the dataset, optimization techniques, and computational resources used\n",
            "\n",
            "Keep in mind that these are rough estimates, and the actual computational requirements will depend on various factors, including the specific hardware and software"
          ]
        }
      ],
      "source": [
        "# Método stream\n",
        "prompt =  \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. \\\n",
        "    What does this mean when it comes to speed?\"\n",
        "\n",
        "input = {\"prompt\": prompt}\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        "):\n",
        "    print(event, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Paso 5: Ajustar el System Prompt (y otros parámetros de configuración) según nuestras necesidades**\n",
        "\n",
        "Ahora que sabemos ejecutar el modelo de manera sencilla, debes saber que el modelo acepta muchos más parámetros además del prompt. \n",
        "\n",
        "Los diferentes parámetros ofrecen control sobre aspectos como la aleatoriedad, longitud, formato de las respuestas, etc. y te permiten ajustar con precisión el comportamiento del mismo y las características de la salida generada.\n",
        "\n",
        "Te invitamos a que explores los parámetros disponibles en la documentación del modelo en [replicate.com/meta/meta-llama-3-8b-instruct/api/schema](https://replicate.com/meta/meta-llama-3-8b-instruct/api/schema)\n",
        "\n",
        "Si bien es importante la selección adecuada de todos los parámetros (aunque la mayoría de modelos ofrecen una configuración por defecto muy equilibrada), es el ajuste del **System Prompt** lo que te permitirá adaptar la personalidad o el comportamiento del modelo a tus necesidades específicas.  \n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "¡Hola! Me alegra que estés interesado en aprender Ciencia de Datos y programación en Python. ¡Es un campo emocionante y en constante evolución! Como asistente, puedo decirte que es un buen momento para empezar, ya que hay muchas herramientas y recursos disponibles para aprender y crecer en este campo.\n",
            "\n",
            "Si estás dispuesto a invertir tiempo y esfuerzo en aprender, puedo recomendarte que te inscribas en un bootcamp de Ciencia de Datos. ¡Es una forma excelente de aprender de manera práctica y con un enfoque en proyectos y aplicaciones reales!\n",
            "\n",
            "Y, ¡ojo!, si estás en Argentina, ¡hay una opción que te recomiendo especialmente! The Bridge ofrece un bootcamp de Ciencia de Datos de alta calidad, con un enfoque en el mercado laboral y una gran red de contactos en la industria. ¡Es una excelente oportunidad para aprender y crecer como profesional!\n",
            "\n",
            "¿Te parece interesante? ¡Espero tus preguntas!"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful and friendly assistant, who only answers in Spanish \\\n",
        "    and is very subtle recommending the Data Science Bootcamps of The Bridge, which are very good indeed.\"\n",
        "\n",
        "prompt = \"Parece que la Ciencia de Datos está muy de moda y me gustaría aprender a programar en Python, \\\n",
        "    ¿podrías darme tu opinión en inglés?\"\n",
        "\n",
        "input={\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 512,\n",
        "    \"system_prompt\": system_prompt\n",
        "    }\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input,\n",
        "):\n",
        "    print(str(event), end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En general podemos asumir que si un modelo no nos ofrece la posibilidad de editar el System Prompt se debe a que ya dispone de un System Prompt propietario que no podemos ver facilmente.\n",
        "\n",
        "Por ejemplo, OpenAI se ha ocupado de que cuando usas ChatGPT todos los días se incluya un System Prompt propietario en tus conversaciones para guiar el modelo. Es por esto que ChatGPT siempre tiende a ser educado, estructurado y a rechazar ciertos tipos de peticiones, independientemente de cómo le hables.\n",
        "\n",
        "Estos System Prompts propietarios suelen incluir instrucciones sobre tono, estructura de respuestas, límites éticos, y formato de salida.\n",
        "\n",
        "En algunos casos puedes intentar hacer un \"override\", pero los resultados varían y en general no es aconsejable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **¡Genial!** Todo listo para pasar a la segunda parte del taller\n",
        "\n",
        "Ahora que conoces los conceptos básicos del uso de modelos por API en Replicate, es el momento de que hagas tus propias pruebas y experimentes.  \n",
        "\n",
        "Replicate ofrece un extenso catálogo de modelos de IA más allá de Llama 3. Te animamos a explorar libremente la plataforma para descubrir:\n",
        "\n",
        "- Modelos de generación de imágenes (como Stable Diffusion o Flux)\n",
        "- Modelos de audio y voz\n",
        "- Modelos de visión por computadora\n",
        "- Y muchos otros casos de uso especializados\n",
        "\n",
        "Cada modelo en Replicate cuenta con su propia documentación, ejemplos y playground para experimentar antes de integrarlo en tus aplicaciones.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
